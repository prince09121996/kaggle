{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T12:59:46.669388Z",
     "iopub.status.busy": "2021-01-09T12:59:46.668451Z",
     "iopub.status.idle": "2021-01-09T12:59:47.546335Z",
     "shell.execute_reply": "2021-01-09T12:59:47.545559Z"
    },
    "papermill": {
     "duration": 0.938081,
     "end_time": "2021-01-09T12:59:47.546481",
     "exception": false,
     "start_time": "2021-01-09T12:59:46.608400",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from numba import njit\n",
    "\n",
    "import collections\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.048227,
     "end_time": "2021-01-09T12:59:47.643912",
     "exception": false,
     "start_time": "2021-01-09T12:59:47.595685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load data\n",
    "\n",
    "Loading a pickle file. Check this notebook [pickling](https://www.kaggle.com/quillio/pickling) if you haven't pickled your data set yet. Check this notebook [one liner to halve your memory usage](https://www.kaggle.com/jorijnsmit/one-liner-to-halve-your-memory-usage) if you want to reduce memory usage before pickling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T12:59:47.750568Z",
     "iopub.status.busy": "2021-01-09T12:59:47.749785Z",
     "iopub.status.idle": "2021-01-09T13:02:33.040907Z",
     "shell.execute_reply": "2021-01-09T13:02:33.039871Z"
    },
    "papermill": {
     "duration": 165.347801,
     "end_time": "2021-01-09T13:02:33.041070",
     "exception": false,
     "start_time": "2021-01-09T12:59:47.693269",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAINING = False\n",
    "USE_FINETUNE = True     \n",
    "FOLDS = 5\n",
    "SEED = 42\n",
    "\n",
    "train = pd.read_csv('../input/jane-street-market-prediction/train.csv')\n",
    "train = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:02:33.145678Z",
     "iopub.status.busy": "2021-01-09T13:02:33.144296Z",
     "iopub.status.idle": "2021-01-09T13:02:33.146418Z",
     "shell.execute_reply": "2021-01-09T13:02:33.146944Z"
    },
    "papermill": {
     "duration": 0.056809,
     "end_time": "2021-01-09T13:02:33.147100",
     "exception": false,
     "start_time": "2021-01-09T13:02:33.090291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data=train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.048936,
     "end_time": "2021-01-09T13:02:33.245602",
     "exception": false,
     "start_time": "2021-01-09T13:02:33.196666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='Moving_Average'></a>\n",
    "# Moving Average (starter)\n",
    "\n",
    "It is a very standard indicator for financial time series. The goal here is to build a demo. Honestly from what I have seen so far, as we have different securities in the data running windows mean doesn't appears to be that usefull for lower windows.\n",
    "If you want to test their importance the usual pandas way of doing that is simply :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:02:33.356314Z",
     "iopub.status.busy": "2021-01-09T13:02:33.355463Z",
     "iopub.status.idle": "2021-01-09T13:02:33.360136Z",
     "shell.execute_reply": "2021-01-09T13:02:33.359544Z"
    },
    "papermill": {
     "duration": 0.058617,
     "end_time": "2021-01-09T13:02:33.360251",
     "exception": false,
     "start_time": "2021-01-09T13:02:33.301634",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Don't launch that as it may consume a lot of memory)\n",
    "#rw = 10000\n",
    "#train_data_rolled = train_data.rolling(window=rw).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.047581,
     "end_time": "2021-01-09T13:02:33.458080",
     "exception": false,
     "start_time": "2021-01-09T13:02:33.410499",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For a streaming algorithm the idea is to build a class that allows to keep track of past values. Largely inspired from this [Stack exchange answer](https://stackoverflow.com/questions/5147378/rolling-variance-algorithm). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:02:33.571265Z",
     "iopub.status.busy": "2021-01-09T13:02:33.570261Z",
     "iopub.status.idle": "2021-01-09T13:02:33.573517Z",
     "shell.execute_reply": "2021-01-09T13:02:33.574057Z"
    },
    "papermill": {
     "duration": 0.067673,
     "end_time": "2021-01-09T13:02:33.574220",
     "exception": false,
     "start_time": "2021-01-09T13:02:33.506547",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class RunningMean:\n",
    "    def __init__(self, WIN_SIZE=20, n_size = 1):\n",
    "        self.n = 0\n",
    "        self.mean = np.zeros(n_size)\n",
    "        self.cum_sum = 0\n",
    "        self.past_value = 0\n",
    "        self.WIN_SIZE = WIN_SIZE\n",
    "        self.windows = collections.deque(maxlen=WIN_SIZE+1)\n",
    "        \n",
    "    def clear(self):\n",
    "        self.n = 0\n",
    "        self.windows.clear()\n",
    "\n",
    "    def push(self, x):\n",
    "        \n",
    "        x = fillna_npwhere_njit(x, self.past_value)\n",
    "        self.past_value = x\n",
    "        \n",
    "        self.windows.append(x)\n",
    "        self.cum_sum += x\n",
    "        \n",
    "        if self.n < self.WIN_SIZE:\n",
    "            self.n += 1\n",
    "            self.mean = self.cum_sum / float(self.n)\n",
    "            \n",
    "        else:\n",
    "            self.cum_sum -= self.windows.popleft()\n",
    "            self.mean = self.cum_sum / float(self.WIN_SIZE)\n",
    "\n",
    "    def get_mean(self):\n",
    "        return self.mean if self.n else np.zeros(n_size)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Current window values: {}\".format(list(self.windows))\n",
    "\n",
    "# Temporary removing njit as it cause many bugs down the line\n",
    "# Problems mainly due to data types, I have to find where I need to constraint types so as not to make njit angry\n",
    "#@njit\n",
    "def fillna_npwhere_njit(array, values):\n",
    "    if np.isnan(array.sum()):\n",
    "        array = np.where(np.isnan(array), values, array)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049168,
     "end_time": "2021-01-09T13:02:33.671408",
     "exception": false,
     "start_time": "2021-01-09T13:02:33.622240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**We can check that it run fast (iterrrows allow to loop trough rows of a data frame as an interable). Here using numpy array instead of pandas datframe allow to go from 1600 it/sec to 9000+ it/seconds when keeping track of ALL 10000 tick lagged means. Using @gogo827jz fillna method allow to breach 10000 it/sec.** \n",
    "\n",
    "**It also shows you how easy it is to use for inference.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:02:33.780497Z",
     "iopub.status.busy": "2021-01-09T13:02:33.778896Z",
     "iopub.status.idle": "2021-01-09T13:02:47.666097Z",
     "shell.execute_reply": "2021-01-09T13:02:47.665497Z"
    },
    "papermill": {
     "duration": 13.94499,
     "end_time": "2021-01-09T13:02:47.666234",
     "exception": false,
     "start_time": "2021-01-09T13:02:33.721244",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [00:13, 7210.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.52992000e+01,  2.30856456e+00,  2.75208485e-04,  2.21278252e-04,\n",
       "        7.71295984e-04,  6.51291141e-04,  6.50637773e-05,  6.48000000e-02,\n",
       "        6.09682722e-01, -6.60141155e-01, -7.83779110e-02, -7.26573777e-02,\n",
       "       -3.99867474e-02, -3.63928714e-02,  8.31655450e-02, -1.34533165e-01,\n",
       "        7.13514660e-01, -4.12347596e-01,  1.57185182e-01, -2.94938169e-01,\n",
       "        3.76541807e-01, -3.05277543e-01,  5.17730749e-01, -5.41750118e-01,\n",
       "        2.27853825e-01,  4.03666615e-02,  5.22760955e-01,  1.88626794e-01,\n",
       "        3.78053556e-01,  9.66058249e-02,  4.96072345e-01,  1.77190446e-01,\n",
       "        5.50528838e-01,  1.99445246e-01,  7.81445987e-02, -5.42174043e-03,\n",
       "        4.10517003e-01,  1.85037518e-01,  2.01603717e-01,  3.11128780e-02,\n",
       "        3.40691709e-01,  1.57422500e-01,  4.28141728e-01,  2.00305715e-01,\n",
       "        1.11706665e-01,  2.04308674e-02, -6.20934679e-02, -9.12123697e-02,\n",
       "        5.68517449e-01,  1.57108281e+00,  3.26757616e+00, -2.13905090e-01,\n",
       "        1.01069183e+00,  1.38320171e+00,  1.38795398e+00,  8.37218409e-01,\n",
       "        9.95779393e-02,  1.10571771e+00,  1.57892476e-01,  3.12495251e-01,\n",
       "        1.06411876e+00, -3.12486190e-02,  1.41574007e+00,  1.48437805e+00,\n",
       "        1.68908040e+00,  1.74712847e+00,  1.65382536e+00,  2.33191573e-01,\n",
       "        2.59479261e-01,  1.17668353e-01,  9.83576746e-02, -2.96530124e-01,\n",
       "        1.53451413e-01,  2.36834432e-01,  1.66753457e-02,  3.25307179e-02,\n",
       "        5.89161835e-01, -6.06805767e-01, -5.16469383e-01, -2.42616899e-01,\n",
       "       -1.01096659e-01, -1.15421932e-01, -1.04510318e-01, -1.11163967e-01,\n",
       "       -1.39422851e-01, -6.29500006e-01, -2.03258507e-01, -2.37376713e-01,\n",
       "       -2.51217944e-01, -2.16132880e-01, -4.24758461e-01,  1.28160920e-01,\n",
       "        9.46310423e-02,  1.66604899e-01, -4.62252825e-03,  3.42098948e-02,\n",
       "       -2.63418479e-01,  1.33020267e+00,  6.53293423e-01,  1.17437625e+00,\n",
       "        9.70667808e-01,  6.48606168e-01,  8.57887311e-01,  2.64785628e-01,\n",
       "        2.47036453e-01,  2.30387936e-01,  2.15120998e-01,  2.16391416e-01,\n",
       "       -1.67259408e-01,  1.70354301e+00,  8.76687330e-01,  1.25571859e+00,\n",
       "        1.13220281e+00,  1.04048066e+00,  9.12730183e-01,  7.65038878e-02,\n",
       "       -4.01472814e-01,  1.63421369e-01,  4.41897226e-02, -3.80330573e-01,\n",
       "       -3.58423631e-01,  1.49004791e+00,  8.10864561e-01,  1.45238995e+00,\n",
       "        1.30659124e+00,  9.43323252e-01,  8.05006342e-01,  1.33048400e+00,\n",
       "        1.57156223e-01,  1.08314098e+00, -1.17035987e-01,  1.05295821e+00,\n",
       "       -1.50339132e-01,  1.33669333e+00, -1.20023879e-01,  1.19718379e+00,\n",
       "       -1.13591499e-01,  9.49995000e+04])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = RunningMean(WIN_SIZE=10000)\n",
    "\n",
    "for index, row in tqdm(train_data[:100000].iterrows()): \n",
    "    a.push(np.array(row))\n",
    "    \n",
    "a.get_mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.101024,
     "end_time": "2021-01-09T13:02:47.869906",
     "exception": false,
     "start_time": "2021-01-09T13:02:47.768882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I still have two main problems here :\n",
    " - <s> It doesn't seem to converge properly due to some rounding error (see below)</s> Thanks to @magokecol for pointing the mistake out !\n",
    " - It does not handle na as is. I propose some code to use the last value at the moment, which might be pertinent for some fetaure but probably not all of them. It is also a bit slower when activated (from 1800 it/sec to 1400 with it/sec).\n",
    " \n",
    "The second point is not problematic as is, but if we want to use it properly we might either want to replicate exactly what standards library (rolling) does or we want to apply the streaming algo to our whole train set, so as not to create a discrepency between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:02:48.107725Z",
     "iopub.status.busy": "2021-01-09T13:02:48.106584Z",
     "iopub.status.idle": "2021-01-09T13:02:48.119145Z",
     "shell.execute_reply": "2021-01-09T13:02:48.116806Z"
    },
    "papermill": {
     "duration": 0.148329,
     "end_time": "2021-01-09T13:02:48.119542",
     "exception": false,
     "start_time": "2021-01-09T13:02:47.971213",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[94.5]\n",
      "94.5\n"
     ]
    }
   ],
   "source": [
    "a = RunningMean(WIN_SIZE=10)\n",
    "\n",
    "for index, row in pd.DataFrame({'col1':range(1,100)}).iterrows(): \n",
    "    a.push(np.array(row))\n",
    "    \n",
    "print(a.get_mean())\n",
    "print((90+91+92+93+94+95+96+97+98+99)/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.101215,
     "end_time": "2021-01-09T13:02:48.321407",
     "exception": false,
     "start_time": "2021-01-09T13:02:48.220192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üòç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.102753,
     "end_time": "2021-01-09T13:02:48.527210",
     "exception": false,
     "start_time": "2021-01-09T13:02:48.424457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='Moving_Moments'></a>\n",
    "# Moving Moments (variance, skew, kurtosis)\n",
    "\n",
    "The aforementionned stack exchange post also implement the variance :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:02:48.741241Z",
     "iopub.status.busy": "2021-01-09T13:02:48.740157Z",
     "iopub.status.idle": "2021-01-09T13:02:48.754084Z",
     "shell.execute_reply": "2021-01-09T13:02:48.753063Z"
    },
    "papermill": {
     "duration": 0.124723,
     "end_time": "2021-01-09T13:02:48.754209",
     "exception": false,
     "start_time": "2021-01-09T13:02:48.629486",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import collections\n",
    "import math\n",
    "\n",
    "\n",
    "class RunningStats:\n",
    "    def __init__(self, WIN_SIZE=20, n_size = 1):\n",
    "        self.n = 0\n",
    "        self.mean = 0\n",
    "        self.run_var = 0\n",
    "        self.WIN_SIZE = WIN_SIZE\n",
    "        self.past_value = 0\n",
    "        self.windows = collections.deque(maxlen=WIN_SIZE+1)\n",
    "\n",
    "    def clear(self):\n",
    "        self.n = 0\n",
    "        self.windows.clear()\n",
    "\n",
    "    def push(self, x):\n",
    "        \n",
    "        x = fillna_npwhere_njit(x, self.past_value)\n",
    "        self.past_value = x\n",
    "\n",
    "        self.windows.append(x)\n",
    "\n",
    "        if self.n < self.WIN_SIZE:\n",
    "            # Calculating first variance\n",
    "            self.n += 1\n",
    "            delta = x - self.mean\n",
    "            self.mean += delta / self.n\n",
    "            self.run_var += delta * (x - self.mean)\n",
    "        else:\n",
    "            # Adjusting variance\n",
    "            x_removed = self.windows.popleft()\n",
    "            old_m = self.mean\n",
    "            self.mean += (x - x_removed) / self.WIN_SIZE\n",
    "            self.run_var += (x + x_removed - old_m - self.mean) * (x - x_removed)\n",
    "\n",
    "    def get_mean(self):\n",
    "        return self.mean if self.n else np.zeros(n_size)\n",
    "\n",
    "    def get_var(self):\n",
    "        return self.run_var / (self.n) if self.n > 1 else np.zeros(n_size)\n",
    "\n",
    "    def get_std(self):\n",
    "        return math.sqrt(self.get_var())\n",
    "\n",
    "    def get_all(self):\n",
    "        return list(self.windows)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Current window values: {}\".format(list(self.windows))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.100734,
     "end_time": "2021-01-09T13:02:48.957802",
     "exception": false,
     "start_time": "2021-01-09T13:02:48.857068",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It isn't really slower than the mean approach. So we might get the variance for almost free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:02:49.172694Z",
     "iopub.status.busy": "2021-01-09T13:02:49.171038Z",
     "iopub.status.idle": "2021-01-09T13:03:03.714175Z",
     "shell.execute_reply": "2021-01-09T13:03:03.713581Z"
    },
    "papermill": {
     "duration": 14.653608,
     "end_time": "2021-01-09T13:03:03.714301",
     "exception": false,
     "start_time": "2021-01-09T13:02:49.060693",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [00:14, 6883.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.52992000e+01,  2.30856456e+00,  2.75208485e-04,  2.21278252e-04,\n",
       "        7.71295984e-04,  6.51291141e-04,  6.50637773e-05,  6.48000000e-02,\n",
       "        6.09682722e-01, -6.60141155e-01, -7.83779110e-02, -7.26573777e-02,\n",
       "       -3.99867474e-02, -3.63928714e-02,  8.31655450e-02, -1.34533165e-01,\n",
       "        7.13514660e-01, -4.12347596e-01,  1.57185182e-01, -2.94938169e-01,\n",
       "        3.76541807e-01, -3.05277543e-01,  5.17730749e-01, -5.41750118e-01,\n",
       "        2.27853825e-01,  4.03666615e-02,  5.22760955e-01,  1.88626794e-01,\n",
       "        3.78053556e-01,  9.66058249e-02,  4.96072345e-01,  1.77190446e-01,\n",
       "        5.50528838e-01,  1.99445246e-01,  7.81445987e-02, -5.42174043e-03,\n",
       "        4.10517003e-01,  1.85037518e-01,  2.01603717e-01,  3.11128780e-02,\n",
       "        3.40691709e-01,  1.57422500e-01,  4.28141728e-01,  2.00305715e-01,\n",
       "        1.11706665e-01,  2.04308674e-02, -6.20934679e-02, -9.12123697e-02,\n",
       "        5.68517449e-01,  1.57108281e+00,  3.26757616e+00, -2.13905090e-01,\n",
       "        1.01069183e+00,  1.38320171e+00,  1.38795398e+00,  8.37218409e-01,\n",
       "        9.95779393e-02,  1.10571771e+00,  1.57892476e-01,  3.12495251e-01,\n",
       "        1.06411876e+00, -3.12486190e-02,  1.41574007e+00,  1.48437805e+00,\n",
       "        1.68908040e+00,  1.74712847e+00,  1.65382536e+00,  2.33191573e-01,\n",
       "        2.59479261e-01,  1.17668353e-01,  9.83576746e-02, -2.96530124e-01,\n",
       "        1.53451413e-01,  2.36834432e-01,  1.66753457e-02,  3.25307179e-02,\n",
       "        5.89161835e-01, -6.06805767e-01, -5.16469383e-01, -2.42616899e-01,\n",
       "       -1.01096659e-01, -1.15421932e-01, -1.04510318e-01, -1.11163967e-01,\n",
       "       -1.39422851e-01, -6.29500006e-01, -2.03258507e-01, -2.37376713e-01,\n",
       "       -2.51217944e-01, -2.16132880e-01, -4.24758461e-01,  1.28160920e-01,\n",
       "        9.46310423e-02,  1.66604899e-01, -4.62252825e-03,  3.42098948e-02,\n",
       "       -2.63418479e-01,  1.33020267e+00,  6.53293423e-01,  1.17437625e+00,\n",
       "        9.70667808e-01,  6.48606168e-01,  8.57887311e-01,  2.64785628e-01,\n",
       "        2.47036453e-01,  2.30387936e-01,  2.15120998e-01,  2.16391416e-01,\n",
       "       -1.67259408e-01,  1.70354301e+00,  8.76687330e-01,  1.25571859e+00,\n",
       "        1.13220281e+00,  1.04048066e+00,  9.12730183e-01,  7.65038878e-02,\n",
       "       -4.01472814e-01,  1.63421369e-01,  4.41897226e-02, -3.80330573e-01,\n",
       "       -3.58423631e-01,  1.49004791e+00,  8.10864561e-01,  1.45238995e+00,\n",
       "        1.30659124e+00,  9.43323252e-01,  8.05006342e-01,  1.33048400e+00,\n",
       "        1.57156223e-01,  1.08314098e+00, -1.17035987e-01,  1.05295821e+00,\n",
       "       -1.50339132e-01,  1.33669333e+00, -1.20023879e-01,  1.19718379e+00,\n",
       "       -1.13591499e-01,  9.49995000e+04])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = RunningStats(WIN_SIZE=10000)\n",
    "\n",
    "for index, row in tqdm(train_data[:100000].iterrows()): \n",
    "    a.push(np.array(row))\n",
    "    \n",
    "a.get_mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.164132,
     "end_time": "2021-01-09T13:03:04.034936",
     "exception": false,
     "start_time": "2021-01-09T13:03:03.870804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I implement the modifications suggested in comments + add some way to handle missing values. As above this is not problematic for the mean. I think it might get a bit more problematic for the variance, as replacing with last values will systematically lower the variance.\n",
    "\n",
    "Note : \n",
    "- I haven't tested the variance toroughfully yet\n",
    "- The post refers to a blog wich refer to another post that gives a solution for [skew and kurtosis in C++](https://www.johndcook.com/blog/skewness_kurtosis/), I'll see what I can implement myself in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.152896,
     "end_time": "2021-01-09T13:03:04.345360",
     "exception": false,
     "start_time": "2021-01-09T13:03:04.192464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For reference I'll leave the pandas implementation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:03:04.655838Z",
     "iopub.status.busy": "2021-01-09T13:03:04.654954Z",
     "iopub.status.idle": "2021-01-09T13:03:04.658856Z",
     "shell.execute_reply": "2021-01-09T13:03:04.658274Z"
    },
    "papermill": {
     "duration": 0.160445,
     "end_time": "2021-01-09T13:03:04.658988",
     "exception": false,
     "start_time": "2021-01-09T13:03:04.498543",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#rw = 10000\n",
    "#train_data_rolled_mean = train_data.rolling(window=rw).mean()\n",
    "#train_data_rolled_var = train_data.rolling(window=rw).var()\n",
    "#train_data_rolled_skew = train_data.rolling(window=rw).skew()\n",
    "#train_data_rolled_kurt = train_data.rolling(window=rw).kurt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.156416,
     "end_time": "2021-01-09T13:03:04.968843",
     "exception": false,
     "start_time": "2021-01-09T13:03:04.812427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='EWMA'></a>\n",
    "# Exponentially Weighted Moving Average\n",
    "\n",
    "Python reference implementation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:03:05.287244Z",
     "iopub.status.busy": "2021-01-09T13:03:05.286389Z",
     "iopub.status.idle": "2021-01-09T13:03:05.290437Z",
     "shell.execute_reply": "2021-01-09T13:03:05.289795Z"
    },
    "papermill": {
     "duration": 0.164825,
     "end_time": "2021-01-09T13:03:05.290565",
     "exception": false,
     "start_time": "2021-01-09T13:03:05.125740",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train_data_ewm = train_data.ewm(span=rw, adjust=True).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.154723,
     "end_time": "2021-01-09T13:03:05.598959",
     "exception": false,
     "start_time": "2021-01-09T13:03:05.444236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Given that exponentially weighted moving average can be calculated iteratively without any memory, I feel like it would generally be better to use such Features. I use the formula alpha = 2 / (N+1) that give the same 'center of mass' as the traditional mean. My implementation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:03:05.926103Z",
     "iopub.status.busy": "2021-01-09T13:03:05.924049Z",
     "iopub.status.idle": "2021-01-09T13:03:05.926850Z",
     "shell.execute_reply": "2021-01-09T13:03:05.927426Z"
    },
    "papermill": {
     "duration": 0.171949,
     "end_time": "2021-01-09T13:03:05.927581",
     "exception": false,
     "start_time": "2021-01-09T13:03:05.755632",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RunningEWMean:\n",
    "    def __init__(self, WIN_SIZE=20, n_size = 1, lt_mean = None):\n",
    "        if lt_mean is not None:\n",
    "            self.s = lt_mean\n",
    "        else:\n",
    "            self.s = np.zeros(n_size)\n",
    "        self.past_value = np.zeros(n_size)\n",
    "        self.alpha = 2 /(WIN_SIZE + 1)\n",
    "\n",
    "    def clear(self):\n",
    "        self.s = 0\n",
    "\n",
    "    def push(self, x):\n",
    "        \n",
    "        x = fillna_npwhere_njit(x, self.past_value)\n",
    "        self.past_value = x\n",
    "        self.s = self.alpha * x + (1 - self.alpha) * self.s\n",
    "        \n",
    "    def get_mean(self):\n",
    "        return self.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.156002,
     "end_time": "2021-01-09T13:03:06.241167",
     "exception": false,
     "start_time": "2021-01-09T13:03:06.085165",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Somehow it seems to also work better than the standard average :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:03:06.575015Z",
     "iopub.status.busy": "2021-01-09T13:03:06.571423Z",
     "iopub.status.idle": "2021-01-09T13:03:06.595248Z",
     "shell.execute_reply": "2021-01-09T13:03:06.596060Z"
    },
    "papermill": {
     "duration": 0.198522,
     "end_time": "2021-01-09T13:03:06.596221",
     "exception": false,
     "start_time": "2021-01-09T13:03:06.397699",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[94.50000001]\n",
      "col1    94.5\n",
      "Name: 98, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "a = RunningEWMean(WIN_SIZE=10)\n",
    "\n",
    "for index, row in pd.DataFrame({'col1':range(1,100)}).iterrows(): \n",
    "    a.push(np.array(row))\n",
    "    \n",
    "print(a.get_mean())\n",
    "print(pd.DataFrame({'col1':range(1,100)}).ewm(span=10, adjust=True).mean().iloc[98])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.157769,
     "end_time": "2021-01-09T13:03:06.911533",
     "exception": false,
     "start_time": "2021-01-09T13:03:06.753764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "And it is also a bit faster (11000 it/second):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:03:07.236960Z",
     "iopub.status.busy": "2021-01-09T13:03:07.235481Z",
     "iopub.status.idle": "2021-01-09T13:03:22.203618Z",
     "shell.execute_reply": "2021-01-09T13:03:22.203019Z"
    },
    "papermill": {
     "duration": 15.136543,
     "end_time": "2021-01-09T13:03:22.203748",
     "exception": false,
     "start_time": "2021-01-09T13:03:07.067205",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [00:14, 6686.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.52112479e+01,  2.32438650e+00,  1.83425957e-04,  1.06079173e-04,\n",
       "        6.97550434e-04,  1.04556747e-03,  3.83668363e-04,  5.30955078e-02,\n",
       "        5.72018466e-01, -6.31857156e-01, -7.68449504e-02, -7.30605322e-02,\n",
       "       -4.47209650e-02, -4.14638625e-02,  1.38057143e-01, -8.71133369e-02,\n",
       "        6.50518311e-01, -4.05000561e-01,  1.43647683e-01, -2.85093958e-01,\n",
       "        3.33373201e-01, -3.02297338e-01,  4.74883928e-01, -5.32888158e-01,\n",
       "        2.79796278e-01,  1.05251485e-01,  4.93500882e-01,  1.84321057e-01,\n",
       "        3.17983647e-01,  8.14777997e-02,  4.28385404e-01,  1.39065807e-01,\n",
       "        5.09885714e-01,  1.89440734e-01,  1.02704843e-01,  1.54245356e-02,\n",
       "        4.29675067e-01,  2.06219535e-01,  2.30638442e-01,  5.68146786e-02,\n",
       "        3.76298523e-01,  1.83423922e-01,  4.66500260e-01,  2.26424454e-01,\n",
       "        6.03962570e-02, -4.74978405e-03, -3.09377607e-02, -7.16762322e-02,\n",
       "        5.99576653e-01,  1.46743983e+00,  3.08617275e+00,  2.19061122e-01,\n",
       "        1.39163630e+00,  1.44772773e+00,  1.28736246e+00,  8.21106786e-01,\n",
       "        1.46748036e-01,  1.27843779e+00,  2.33511228e-01,  3.52861000e-01,\n",
       "        1.05475940e+00,  3.24357807e-02,  1.58601139e+00,  1.66245697e+00,\n",
       "        1.76634306e+00,  1.86097302e+00,  1.77410088e+00,  2.37949821e-01,\n",
       "        2.64951718e-01,  1.68932267e-01,  1.52526781e-01, -2.32786219e-01,\n",
       "        2.19468599e-01,  2.82659740e-01,  2.11449774e-03,  1.86708876e-02,\n",
       "        5.29269087e-01, -5.96452178e-01, -5.06986146e-01, -2.51861763e-01,\n",
       "       -8.02523277e-02, -1.27323872e-01, -1.11390476e-01, -9.78537407e-02,\n",
       "       -2.45295774e-01, -5.84862497e-01, -1.68911268e-01, -2.63863004e-01,\n",
       "       -2.81634982e-01, -2.03560263e-01, -5.65063570e-01,  2.84552518e-01,\n",
       "        1.09338809e-01,  2.80539653e-01,  9.53198714e-02,  7.38763075e-02,\n",
       "       -1.60569723e-01,  1.65753753e+00,  6.84294064e-01,  1.38000781e+00,\n",
       "        1.11991573e+00,  7.10249084e-01,  1.01809278e+00,  4.43194876e-01,\n",
       "        2.25274527e-01,  3.74984473e-01,  2.91836387e-01,  2.29704803e-01,\n",
       "       -5.44952843e-02,  2.04417357e+00,  8.38123404e-01,  1.46458260e+00,\n",
       "        1.29056870e+00,  1.09096415e+00,  1.09834666e+00,  2.99725564e-01,\n",
       "       -4.34890506e-01,  2.91365012e-01,  1.23361343e-01, -3.63264425e-01,\n",
       "       -2.23290045e-01,  1.81933972e+00,  7.82414768e-01,  1.69170995e+00,\n",
       "        1.48866634e+00,  9.97736325e-01,  9.46810347e-01,  1.28373815e+00,\n",
       "        1.98696572e-01,  1.06031446e+00, -4.40064341e-02,  1.01905775e+00,\n",
       "       -8.13859956e-02,  1.29243648e+00, -6.38253473e-02,  1.16774793e+00,\n",
       "       -4.92141444e-02,  9.49995000e+04])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = RunningEWMean(WIN_SIZE=10000)\n",
    "\n",
    "for index, row in tqdm(train_data[:100000].iterrows()): \n",
    "    a.push(np.array(row))\n",
    "    \n",
    "a.get_mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.213376,
     "end_time": "2021-01-09T13:03:22.628448",
     "exception": false,
     "start_time": "2021-01-09T13:03:22.415072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='PDA'></a>\n",
    "# Past day average\n",
    "\n",
    "Given that some long term moving average appears to have some gain, I figured it would probably make sense to calculate past day average.\n",
    "I haven't given a lot of attention with a pythonic way but I think I can give it a shot in a streaming way :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:03:23.068546Z",
     "iopub.status.busy": "2021-01-09T13:03:23.067747Z",
     "iopub.status.idle": "2021-01-09T13:03:23.072954Z",
     "shell.execute_reply": "2021-01-09T13:03:23.072389Z"
    },
    "papermill": {
     "duration": 0.231221,
     "end_time": "2021-01-09T13:03:23.073101",
     "exception": false,
     "start_time": "2021-01-09T13:03:22.841880",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RunningPDA:\n",
    "    def __init__(self):\n",
    "        self.day = -1\n",
    "        self.past_mean = 0\n",
    "        self.cum_sum = 0\n",
    "        self.day_instances = 0\n",
    "        self.past_value = 0\n",
    "\n",
    "    def clear(self):\n",
    "        self.n = 0\n",
    "        self.windows.clear()\n",
    "\n",
    "    def push(self, x, date):\n",
    "        \n",
    "        x = fillna_npwhere_njit(x, self.past_value)\n",
    "        self.past_value = x\n",
    "        \n",
    "        # change of day\n",
    "        if date>self.day:\n",
    "            self.day = date\n",
    "            if self.day_instances > 0:\n",
    "                self.past_mean = self.cum_sum/self.day_instances\n",
    "            else:\n",
    "                self.past_mean = 0\n",
    "            self.day_instances = 1\n",
    "            self.cum_sum = x\n",
    "            \n",
    "        else:\n",
    "            self.day_instances += 1\n",
    "            self.cum_sum += x\n",
    "\n",
    "    def get_mean(self):\n",
    "        return self.cum_sum/self.day_instances\n",
    "\n",
    "    def get_past_mean(self):\n",
    "        return self.past_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.212346,
     "end_time": "2021-01-09T13:03:23.497692",
     "exception": false,
     "start_time": "2021-01-09T13:03:23.285346",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A test seems to run pretty fast (10000 it/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:03:23.934991Z",
     "iopub.status.busy": "2021-01-09T13:03:23.933335Z",
     "iopub.status.idle": "2021-01-09T13:03:38.046257Z",
     "shell.execute_reply": "2021-01-09T13:03:38.045300Z"
    },
    "papermill": {
     "duration": 14.336215,
     "end_time": "2021-01-09T13:03:38.046382",
     "exception": false,
     "start_time": "2021-01-09T13:03:23.710167",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [00:14, 7088.82it/s]\n"
     ]
    }
   ],
   "source": [
    "a = RunningPDA()\n",
    "\n",
    "for index, row in tqdm(train_data[:100000].iterrows()): \n",
    "    date=row['date']\n",
    "    a.push(np.array(row),date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:03:38.613398Z",
     "iopub.status.busy": "2021-01-09T13:03:38.612639Z",
     "iopub.status.idle": "2021-01-09T13:03:38.618666Z",
     "shell.execute_reply": "2021-01-09T13:03:38.619185Z"
    },
    "papermill": {
     "duration": 0.281635,
     "end_time": "2021-01-09T13:03:38.619334",
     "exception": false,
     "start_time": "2021-01-09T13:03:38.337699",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.50000000e+01,  2.41088269e+00,  4.03728869e-04,  3.85944943e-04,\n",
       "        9.25485097e-04,  1.97937772e-04, -3.39575249e-04,  7.65223054e-02,\n",
       "        6.59973389e-01, -6.87933667e-01, -7.22627089e-02, -6.60324615e-02,\n",
       "       -2.10331397e-02, -2.29636286e-02,  3.50614645e-02, -1.77169031e-01,\n",
       "        7.58837109e-01, -4.42502048e-01,  1.78205628e-01, -3.05370185e-01,\n",
       "        4.21960741e-01, -3.20471451e-01,  5.52219384e-01, -5.77002928e-01,\n",
       "        1.06999287e-01, -8.39559722e-02,  5.34741730e-01,  1.78753813e-01,\n",
       "        4.28869587e-01,  1.06534202e-01,  5.64061492e-01,  2.18380680e-01,\n",
       "        5.83891154e-01,  2.03274294e-01,  1.07479484e-01,  2.26506158e-02,\n",
       "        3.70229722e-01,  1.50077151e-01,  1.76475659e-01,  1.70292867e-02,\n",
       "        2.96138467e-01,  1.28235596e-01,  3.61975534e-01,  1.57764427e-01,\n",
       "        1.84003945e-01,  5.97135267e-02, -1.03904432e-01, -1.15470425e-01,\n",
       "        4.73092292e-01,  1.73822410e+00,  3.50223899e+00, -8.63688957e-01,\n",
       "        3.72510916e-01,  1.45294095e+00,  1.65563801e+00,  1.02878348e+00,\n",
       "        1.78714434e-01,  1.08848941e+00,  1.64132964e-01,  2.41613440e-01,\n",
       "        9.36217853e-01, -2.71332042e-01,  1.14026454e+00,  1.14151247e+00,\n",
       "        1.43639133e+00,  1.40148647e+00,  1.40259854e+00,  6.35899176e-01,\n",
       "        6.28986881e-01,  5.89303632e-01,  5.89307578e-01,  5.98412561e-01,\n",
       "        7.14365300e-01,  6.95432155e-01,  7.75052922e-01,  7.73414506e-01,\n",
       "        6.73337446e-01, -5.92769774e-01, -5.22474297e-01, -1.09353802e-01,\n",
       "       -1.28482066e-01, -7.68803003e-02, -9.58586668e-02, -1.27234741e-01,\n",
       "       -8.74771430e-02, -4.06412660e-01, -2.42054115e-01, -1.76915799e-01,\n",
       "       -2.09855644e-01, -2.21748333e-01, -4.45212818e-01, -5.49872306e-02,\n",
       "        1.08958221e-01,  2.72687444e-02, -1.15886147e-01,  1.64563245e-02,\n",
       "        1.99175239e-02,  9.12427765e-01,  6.60192648e-01,  7.77876325e-01,\n",
       "        6.76600309e-01,  5.69659214e-01,  1.20770301e+00, -1.88498397e-02,\n",
       "        3.09359353e-01,  2.75269402e-02,  1.27745812e-01,  2.25845782e-01,\n",
       "        6.80464226e-02,  1.16540872e+00,  9.53897550e-01,  8.51191116e-01,\n",
       "        8.38015091e-01,  9.58454557e-01,  1.25756190e+00, -2.42803267e-01,\n",
       "       -2.54614178e-01, -2.37029989e-03, -3.66435888e-02, -3.15857958e-01,\n",
       "       -2.74225192e-02,  1.02080545e+00,  9.28342083e-01,  9.93005461e-01,\n",
       "        9.69916201e-01,  8.93247617e-01,  1.08765444e+00,  9.95649052e-01,\n",
       "       -5.11492814e-01,  9.37370832e-01, -4.50914399e-01,  8.87992510e-01,\n",
       "       -7.20172094e-01,  1.14117623e+00, -5.03714461e-01,  1.03294797e+00,\n",
       "       -4.38338194e-01,  9.35035000e+04])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_past_mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.265059,
     "end_time": "2021-01-09T13:03:39.149234",
     "exception": false,
     "start_time": "2021-01-09T13:03:38.884175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Which seems to match (weight 2.7143664 match the first value on the first row) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:03:39.713688Z",
     "iopub.status.busy": "2021-01-09T13:03:39.712025Z",
     "iopub.status.idle": "2021-01-09T13:03:40.047901Z",
     "shell.execute_reply": "2021-01-09T13:03:40.048651Z"
    },
    "papermill": {
     "duration": 0.634127,
     "end_time": "2021-01-09T13:03:40.048814",
     "exception": false,
     "start_time": "2021-01-09T13:03:39.414687",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weight              2.714364\n",
       "resp_1             -0.000117\n",
       "resp_2             -0.000314\n",
       "resp_3             -0.001018\n",
       "resp_4             -0.002439\n",
       "                   ...      \n",
       "feature_126         0.657784\n",
       "feature_127         0.546017\n",
       "feature_128         0.640876\n",
       "feature_129         0.552540\n",
       "ts_id          193645.000000\n",
       "Name: 30, Length: 137, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:200000].groupby('date').mean().iloc[30,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.263786,
     "end_time": "2021-01-09T13:03:40.579483",
     "exception": false,
     "start_time": "2021-01-09T13:03:40.315697",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Great news !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.26065,
     "end_time": "2021-01-09T13:03:41.103382",
     "exception": false,
     "start_time": "2021-01-09T13:03:40.842732",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='PTI'></a>\n",
    "# Previous trade information from the same underlying\n",
    "\n",
    "As mentionned [here](https://www.kaggle.com/c/jane-street-market-prediction/discussion/207709) feature_41 being constant over the day allow to find the previous instance with the same feature_41 caracteristic. As we don't exactly know what are in those feature we can't really know how the trade opportunities relate exactly but I speculate that they relate to the same underlying or are pretty close and that information about the previous trade of the day relating to the same underlying might be usefull. At the moment the implementation rely on a simple dictionnary. I suspect I can't really get below keeping 800-900 instances in memory for that feature engineering technique as I need at least one example of each trade. It is not a problem and the method is really fast (10000 it/s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:03:41.887367Z",
     "iopub.status.busy": "2021-01-09T13:03:41.886370Z",
     "iopub.status.idle": "2021-01-09T13:03:41.892839Z",
     "shell.execute_reply": "2021-01-09T13:03:41.893532Z"
    },
    "papermill": {
     "duration": 0.444558,
     "end_time": "2021-01-09T13:03:41.893720",
     "exception": false,
     "start_time": "2021-01-09T13:03:41.449162",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RunningPTI:\n",
    "    def __init__(self,base_value=0):\n",
    "        self.dictionnary = {}\n",
    "        self.base_value = base_value\n",
    "        self.day = -1\n",
    "\n",
    "    def clear(self):\n",
    "        self.dictionnary = {}\n",
    "        self.base_value = 0\n",
    "\n",
    "    def push(self, x, value, date):\n",
    "        \n",
    "                # change of day\n",
    "        if date>self.day:\n",
    "            self.day = date\n",
    "            self.dictionnary = {}\n",
    "        \n",
    "        self.past_value = self.dictionnary.get(value)\n",
    "        self.dictionnary.update({value:x})\n",
    "        \n",
    "    def get_past_value(self):\n",
    "        if self.past_value is None:\n",
    "            self.past_value = self.base_value\n",
    "        return self.past_value\n",
    "    \n",
    "    def get_dict(self):\n",
    "        return self.dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:03:42.641036Z",
     "iopub.status.busy": "2021-01-09T13:03:42.640010Z",
     "iopub.status.idle": "2021-01-09T13:03:43.833697Z",
     "shell.execute_reply": "2021-01-09T13:03:43.833111Z"
    },
    "papermill": {
     "duration": 1.575591,
     "end_time": "2021-01-09T13:03:43.833811",
     "exception": false,
     "start_time": "2021-01-09T13:03:42.258220",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:01, 8537.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.00000000e+00,  0.00000000e+00,  3.28692608e-03,  5.02428086e-03,\n",
       "        9.49510094e-03,  1.63225569e-02,  1.22323101e-02,  1.00000000e+00,\n",
       "        1.10244071e+00,  4.39071143e-03,  6.34364843e-01,  3.01364392e-01,\n",
       "       -1.82812855e-01, -1.15289658e-01,             nan,             nan,\n",
       "        2.77707505e+00,  9.81782436e-01,  1.83376551e+00,  1.80425596e+00,\n",
       "        2.67071676e+00,  1.33451498e+00,  2.15938163e+00,  1.39397800e+00,\n",
       "                   nan,             nan,  2.61860538e+00,  2.00022745e+00,\n",
       "        4.21787024e+00,  2.11934543e+00,  2.99862647e+00,  2.54606199e+00,\n",
       "        3.04978371e+00,  2.28271413e+00,             nan,             nan,\n",
       "       -8.34699750e-01, -6.48146331e-01, -3.43272030e-01, -2.80105948e-01,\n",
       "       -6.42717481e-01, -3.26380342e-01, -1.24539757e+00, -6.11685693e-01,\n",
       "        1.44464087e+00,  9.83209491e-01, -5.26694618e-02, -3.35607119e-02,\n",
       "       -2.46033356e-01,  1.24928570e+00,  2.31220937e+00, -1.84495616e+00,\n",
       "       -7.37610936e-01, -4.35770541e-01, -4.34156731e-02, -4.41417992e-01,\n",
       "       -6.50321186e-01, -1.05066681e+00, -9.81512547e-01, -6.15516722e-01,\n",
       "        2.53980613e+00,  1.77891731e+00, -8.57290328e-01, -3.32437634e-01,\n",
       "       -3.38077903e-01, -1.85289513e-02, -7.38057196e-01, -8.84314656e-01,\n",
       "       -7.42745101e-01, -1.59235314e-01, -1.75652266e-01, -2.44836521e+00,\n",
       "       -1.00371659e+00, -7.13986337e-01, -4.13072109e+00, -3.91822243e+00,\n",
       "        1.73565078e+00,  5.68383634e-01,  4.43563581e-01,             nan,\n",
       "       -6.52917206e-01, -6.84055924e-01, -8.61839592e-01, -6.12519622e-01,\n",
       "       -2.89222509e-01,             nan, -3.23971272e-01, -6.45738423e-01,\n",
       "       -9.16148782e-01, -2.98564106e-01, -2.05677330e-01,             nan,\n",
       "       -1.23821247e+00, -9.29209769e-01, -1.66108179e+00, -1.64186013e+00,\n",
       "       -1.98990285e+00,             nan, -1.51561344e+00, -1.49061823e+00,\n",
       "       -1.32274330e+00, -1.08688641e+00, -2.70095515e+00,             nan,\n",
       "        6.76056862e-01,  9.03004333e-02,  3.51407051e-01,  2.47029111e-01,\n",
       "       -1.46934223e+00,             nan,  1.41195133e-01, -2.28041187e-01,\n",
       "        2.34258231e-02, -2.30946556e-01, -2.32916999e+00,             nan,\n",
       "       -8.12913418e-01, -4.26460236e-01, -2.93363065e-01, -1.19232559e+00,\n",
       "       -2.29696417e+00,             nan, -1.38871062e+00, -1.12619746e+00,\n",
       "       -1.08326030e+00, -1.77898860e+00, -1.79304671e+00,  1.60383737e+00,\n",
       "        1.33373523e+00,  5.12490869e-01, -8.91093910e-01,  1.29215169e+00,\n",
       "        9.72259045e-01,  1.49736404e+00,  2.29763985e-01,  8.06939781e-01,\n",
       "       -4.73931789e-01,  6.42900000e+03])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = RunningPTI(base_value=np.nan * np.empty((1, 138)))\n",
    "\n",
    "for index, row in tqdm(train_data[:10000].iterrows()): \n",
    "    f_41 = row['feature_41']\n",
    "    date = row['date']\n",
    "    a.push(np.array(row),f_41,date)\n",
    "    \n",
    "a.get_past_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.269955,
     "end_time": "2021-01-09T13:03:44.382367",
     "exception": false,
     "start_time": "2021-01-09T13:03:44.112412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Note : I haven't toroughfully tested it yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.269843,
     "end_time": "2021-01-09T13:03:44.920867",
     "exception": false,
     "start_time": "2021-01-09T13:03:44.651024",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='DIFF'></a>\n",
    "# Differentiation\n",
    "\n",
    "Direct and second order differentation of averaged variables seems to have some importance, as is, change in overall trends have an importance for the problem of dealing with multiple securities. To be more clear :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:03:45.717875Z",
     "iopub.status.busy": "2021-01-09T13:03:45.716795Z",
     "iopub.status.idle": "2021-01-09T13:03:45.719587Z",
     "shell.execute_reply": "2021-01-09T13:03:45.718760Z"
    },
    "papermill": {
     "duration": 0.414449,
     "end_time": "2021-01-09T13:03:45.719734",
     "exception": false,
     "start_time": "2021-01-09T13:03:45.305285",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#rw = 10000\n",
    "#train_data_diff = train_data.rolling(window=rw).mean().diff(rw)\n",
    "#train_data_diff_diff = train_data.rolling(window=rw).mean().diff(rw).diff(rw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.334927,
     "end_time": "2021-01-09T13:03:46.443804",
     "exception": false,
     "start_time": "2021-01-09T13:03:46.108877",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Seems to retain some importance when an xgboost is calibrated on them, especially for higher rw. I haven't built a running algo for them yet, but I'll probably try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.265821,
     "end_time": "2021-01-09T13:03:46.984049",
     "exception": false,
     "start_time": "2021-01-09T13:03:46.718228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='FDIFF'></a>\n",
    "# Fractional differentiation\n",
    "\n",
    "Very important feature engineering tool, especially for time series *. As illustrated by Marcos Lopez de Prado in his book Advances in Financial Machine Learning, It is a great tool to remove noise without removing information. \n",
    "\n",
    "The idea is to generalise differentiation to non integer. Applying multiple stationnarity tests while slowly increasing the fractionnal differentiation level, you can get an 'optimal' level (enough differentiation to remove noise, without removing information). \n",
    "\n",
    "However, I havea found that for optimal level of around -0.75 - that I found in the dataset ** - we would need longer series to make the calculation meaningful. This is illustrated in the example below : the weight for the millionth instance is still above 0.02 for the first one. This is also problematic as it means that the first instances would lack a lot of information.\n",
    "\n",
    "\\* : it is rather difficult to even use time series tools here as we have many underlying securities\n",
    "\n",
    "**  : when the adf test would even converge after lenghty calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:03:47.534708Z",
     "iopub.status.busy": "2021-01-09T13:03:47.533819Z",
     "iopub.status.idle": "2021-01-09T13:03:47.538191Z",
     "shell.execute_reply": "2021-01-09T13:03:47.537635Z"
    },
    "papermill": {
     "duration": 0.282473,
     "end_time": "2021-01-09T13:03:47.538299",
     "exception": false,
     "start_time": "2021-01-09T13:03:47.255826",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_weights(d, size):\n",
    "    w = [1.]\n",
    "    for k in range(1, size):\n",
    "        w_ = -w[-1] / k * (d - k + 1)\n",
    "        w.append(w_)\n",
    "    w = np.array(w[::-1]).reshape(-1, 1)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.27479,
     "end_time": "2021-01-09T13:03:48.082933",
     "exception": false,
     "start_time": "2021-01-09T13:03:47.808143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So, unless someone points out a way to make fractionnal differentiation work in our data set (or that an approximation allows for some information gain), I don't really plan to make a streaming algorithm for building it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.273634,
     "end_time": "2021-01-09T13:03:48.645446",
     "exception": false,
     "start_time": "2021-01-09T13:03:48.371812",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='Entropy'></a>\n",
    "# Entropy rate\n",
    "\n",
    "Also mentionned in lopez de prado's book. It relate to some physical mesure of order. I am still not entirely convinced this could work here. Especially because there is a lot of different notions of entropy and all of them are rather calculatory. For reference (outside of inference) I was able to calculate entropy on sliding windows with the pyinform package (see code below). But this is rather slow and doesn't seems to provide any information gain in my early tests.\n",
    "\n",
    "However I have found [a streaming implementation](https://github.com/ajcr/rolling) that could be reimplemented for our problem so I am mentionning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:03:49.196878Z",
     "iopub.status.busy": "2021-01-09T13:03:49.195527Z",
     "iopub.status.idle": "2021-01-09T13:03:49.199730Z",
     "shell.execute_reply": "2021-01-09T13:03:49.198806Z"
    },
    "papermill": {
     "duration": 0.280357,
     "end_time": "2021-01-09T13:03:49.199862",
     "exception": false,
     "start_time": "2021-01-09T13:03:48.919505",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install pyinform\n",
    "#from pyinform import entropy_rate\n",
    "\n",
    "#entropy_r = lambda x: entropy_rate(x,k=2)\n",
    "\n",
    "#df[feature] = (df[feature] > df[feature].mean())\n",
    "#df[feature] = df[feature].rolling(window=rw[i]).apply(entropy_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.267797,
     "end_time": "2021-01-09T13:03:49.733179",
     "exception": false,
     "start_time": "2021-01-09T13:03:49.465382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='MLP'></a>\n",
    "# Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.267706,
     "end_time": "2021-01-09T13:03:50.270693",
     "exception": false,
     "start_time": "2021-01-09T13:03:50.002987",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this section I give you an idea of one would apply the algo for submission.\n",
    "The model used come from this [notebook](https://www.kaggle.com/aimind/bottleneck-encoder-mlp-keras-tuner-8601c5). I only show where feature engineering appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.272426,
     "end_time": "2021-01-09T13:03:50.813287",
     "exception": false,
     "start_time": "2021-01-09T13:03:50.540861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loading the packages :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:03:51.374993Z",
     "iopub.status.busy": "2021-01-09T13:03:51.374229Z",
     "iopub.status.idle": "2021-01-09T13:03:58.658731Z",
     "shell.execute_reply": "2021-01-09T13:03:58.659347Z"
    },
    "papermill": {
     "duration": 7.571298,
     "end_time": "2021-01-09T13:03:58.659529",
     "exception": false,
     "start_time": "2021-01-09T13:03:51.088231",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "from random import choices\n",
    "\n",
    "\n",
    "import kerastuner as kt\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "          tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "          # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.270315,
     "end_time": "2021-01-09T13:03:59.205078",
     "exception": false,
     "start_time": "2021-01-09T13:03:58.934763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "PurgedTimeSeries CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:03:59.777764Z",
     "iopub.status.busy": "2021-01-09T13:03:59.770879Z",
     "iopub.status.idle": "2021-01-09T13:03:59.781331Z",
     "shell.execute_reply": "2021-01-09T13:03:59.780268Z"
    },
    "papermill": {
     "duration": 0.305409,
     "end_time": "2021-01-09T13:03:59.781470",
     "exception": false,
     "start_time": "2021-01-09T13:03:59.476061",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:04:00.338645Z",
     "iopub.status.busy": "2021-01-09T13:04:00.330941Z",
     "iopub.status.idle": "2021-01-09T13:04:00.341120Z",
     "shell.execute_reply": "2021-01-09T13:04:00.341698Z"
    },
    "papermill": {
     "duration": 0.291106,
     "end_time": "2021-01-09T13:04:00.341848",
     "exception": false,
     "start_time": "2021-01-09T13:04:00.050742",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CVTuner(kt.engine.tuner.Tuner):\n",
    "    def run_trial(self, trial, X, y, splits, batch_size=32, epochs=1,callbacks=None):\n",
    "        val_losses = []\n",
    "        for train_indices, test_indices in splits:\n",
    "            X_train, X_test = [x[train_indices] for x in X], [x[test_indices] for x in X]\n",
    "            y_train, y_test = [a[train_indices] for a in y], [a[test_indices] for a in y]\n",
    "            if len(X_train) < 2:\n",
    "                X_train = X_train[0]\n",
    "                X_test = X_test[0]\n",
    "            if len(y_train) < 2:\n",
    "                y_train = y_train[0]\n",
    "                y_test = y_test[0]\n",
    "            \n",
    "            model = self.hypermodel.build(trial.hyperparameters)\n",
    "            hist = model.fit(X_train,y_train,\n",
    "                      validation_data=(X_test,y_test),\n",
    "                      epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                      callbacks=callbacks)\n",
    "            \n",
    "            val_losses.append([hist.history[k][-1] for k in hist.history])\n",
    "        val_losses = np.asarray(val_losses)\n",
    "        self.oracle.update_trial(trial.trial_id, {k:np.mean(val_losses[:,i]) for i,k in enumerate(hist.history.keys())})\n",
    "        self.save_model(trial.trial_id, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:04:00.900739Z",
     "iopub.status.busy": "2021-01-09T13:04:00.895400Z",
     "iopub.status.idle": "2021-01-09T13:04:00.904072Z",
     "shell.execute_reply": "2021-01-09T13:04:00.903515Z"
    },
    "papermill": {
     "duration": 0.294328,
     "end_time": "2021-01-09T13:04:00.904196",
     "exception": false,
     "start_time": "2021-01-09T13:04:00.609868",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From https://medium.com/@micwurm/using-tensorflow-lite-to-speed-up-predictions-a3954886eb98\n",
    "\n",
    "class LiteModel:\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, model_path):\n",
    "        return LiteModel(tf.lite.Interpreter(model_path=model_path))\n",
    "    \n",
    "    @classmethod\n",
    "    def from_keras_model(cls, kmodel):\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(kmodel)\n",
    "        tflite_model = converter.convert()\n",
    "        return LiteModel(tf.lite.Interpreter(model_content=tflite_model))\n",
    "    \n",
    "    def __init__(self, interpreter):\n",
    "        self.interpreter = interpreter\n",
    "        self.interpreter.allocate_tensors()\n",
    "        input_det = self.interpreter.get_input_details()[0]\n",
    "        output_det = self.interpreter.get_output_details()[0]\n",
    "        self.input_index = input_det[\"index\"]\n",
    "        self.output_index = output_det[\"index\"]\n",
    "        self.input_shape = input_det[\"shape\"]\n",
    "        self.output_shape = output_det[\"shape\"]\n",
    "        self.input_dtype = input_det[\"dtype\"]\n",
    "        self.output_dtype = output_det[\"dtype\"]\n",
    "        \n",
    "    def predict(self, inp):\n",
    "        inp = inp.astype(self.input_dtype)\n",
    "        count = inp.shape[0]\n",
    "        out = np.zeros((count, self.output_shape[1]), dtype=self.output_dtype)\n",
    "        for i in range(count):\n",
    "            self.interpreter.set_tensor(self.input_index, inp[i:i+1])\n",
    "            self.interpreter.invoke()\n",
    "            out[i] = self.interpreter.get_tensor(self.output_index)[0]\n",
    "        return out\n",
    "    \n",
    "    def predict_single(self, inp):\n",
    "        \"\"\" Like predict(), but only for a single record. The input data can be a Python list. \"\"\"\n",
    "        inp = np.array([inp], dtype=self.input_dtype)\n",
    "        self.interpreter.set_tensor(self.input_index, inp)\n",
    "        self.interpreter.invoke()\n",
    "        out = self.interpreter.get_tensor(self.output_index)\n",
    "        return out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.30615,
     "end_time": "2021-01-09T13:04:01.484914",
     "exception": false,
     "start_time": "2021-01-09T13:04:01.178764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loading the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:04:02.036812Z",
     "iopub.status.busy": "2021-01-09T13:04:02.035747Z",
     "iopub.status.idle": "2021-01-09T13:04:02.109472Z",
     "shell.execute_reply": "2021-01-09T13:04:02.108154Z"
    },
    "papermill": {
     "duration": 0.351022,
     "end_time": "2021-01-09T13:04:02.109613",
     "exception": false,
     "start_time": "2021-01-09T13:04:01.758591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nb_trade = train.groupby('date')['date'].count()\n",
    "high_volume_days = [i for i, x in enumerate(np.array(nb_trade > 7000)) if x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.274832,
     "end_time": "2021-01-09T13:04:02.660439",
     "exception": false,
     "start_time": "2021-01-09T13:04:02.385607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I change the days a bit to remove the day previous day 85 as the continuity of JS strategy is in question (or is that a different market regime ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:04:03.238615Z",
     "iopub.status.busy": "2021-01-09T13:04:03.237460Z",
     "iopub.status.idle": "2021-01-09T13:04:06.899026Z",
     "shell.execute_reply": "2021-01-09T13:04:06.898090Z"
    },
    "papermill": {
     "duration": 3.962665,
     "end_time": "2021-01-09T13:04:06.899178",
     "exception": false,
     "start_time": "2021-01-09T13:04:02.936513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.query('date > 85').reset_index(drop = True) \n",
    "train = train.query('date not in @high_volume_days').reset_index(drop = True) \n",
    "train.fillna(train.mean(),inplace=True)\n",
    "train = train.query('weight > 0').reset_index(drop = True)\n",
    "#train['action'] = (train['resp'] > 0).astype('int')\n",
    "train['action'] =  (  (train['resp_1'] > 0.00001 ) & (train['resp_2'] > 0.00001 ) & (train['resp_3'] > 0.00001 ) & (train['resp_4'] > 0.00001 ) &  (train['resp'] > 0.00001 )   ).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.279415,
     "end_time": "2021-01-09T13:04:07.455298",
     "exception": false,
     "start_time": "2021-01-09T13:04:07.175883",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Adding some feature (long term exponentially weighted mean of feature_0 and feature_1) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:04:08.443237Z",
     "iopub.status.busy": "2021-01-09T13:04:08.441700Z",
     "iopub.status.idle": "2021-01-09T13:09:56.490658Z",
     "shell.execute_reply": "2021-01-09T13:09:56.491531Z"
    },
    "papermill": {
     "duration": 348.470878,
     "end_time": "2021-01-09T13:09:56.491786",
     "exception": false,
     "start_time": "2021-01-09T13:04:08.020908",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1426557it [05:42, 4161.13it/s]\n"
     ]
    }
   ],
   "source": [
    "EWM_5000 = RunningEWMean(WIN_SIZE = 5000)\n",
    "EWM_10000 = RunningEWMean(WIN_SIZE = 10000)\n",
    "EWM_20000 = RunningEWMean(WIN_SIZE = 20000)\n",
    "\n",
    "train_FE = []\n",
    "\n",
    "for index, row in tqdm(train[['feature_0','feature_1']].iterrows()): \n",
    "    EWM_5000.push(np.float64(np.array(row)))\n",
    "    EWM_10000.push(np.float64(np.array(row)))\n",
    "    EWM_20000.push(np.float64(np.array(row)))\n",
    "\n",
    "    FE = {\n",
    "        'feature_0_EWM_5000' : EWM_5000.get_mean()[0],\n",
    "        'feature_1_EWM_5000' : EWM_5000.get_mean()[1],\n",
    "        'feature_0_EWM_10000' : EWM_10000.get_mean()[0],\n",
    "        'feature_1_EWM_10000' : EWM_10000.get_mean()[1],\n",
    "        'feature_0_EWM_20000' : EWM_20000.get_mean()[0],\n",
    "        'feature_1_EWM_20000' : EWM_20000.get_mean()[1],\n",
    "    }\n",
    "\n",
    "    train_FE.append(FE)\n",
    "\n",
    "train_FE = pd.DataFrame(train_FE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.530439,
     "end_time": "2021-01-09T13:09:59.538684",
     "exception": false,
     "start_time": "2021-01-09T13:09:58.008245",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "merge two dataframes and add columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:10:02.588171Z",
     "iopub.status.busy": "2021-01-09T13:10:02.586807Z",
     "iopub.status.idle": "2021-01-09T13:10:05.839318Z",
     "shell.execute_reply": "2021-01-09T13:10:05.839885Z"
    },
    "papermill": {
     "duration": 4.797608,
     "end_time": "2021-01-09T13:10:05.840070",
     "exception": false,
     "start_time": "2021-01-09T13:10:01.042462",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.concat([train,train_FE],axis=1)\n",
    "\n",
    "features = [c for c in train.columns if 'feature' in c]\n",
    "\n",
    "resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n",
    "\n",
    "X = train[features].values\n",
    "y = np.stack([(train[c] > 0.000001).astype('int') for c in resp_cols]).T #Multitarget\n",
    "\n",
    "f_mean = np.mean(train[features[1:]].values,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.796758,
     "end_time": "2021-01-09T13:10:09.580465",
     "exception": false,
     "start_time": "2021-01-09T13:10:07.783707",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Create autoencoder, MLP :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:10:12.918601Z",
     "iopub.status.busy": "2021-01-09T13:10:12.917523Z",
     "iopub.status.idle": "2021-01-09T13:10:12.921217Z",
     "shell.execute_reply": "2021-01-09T13:10:12.920629Z"
    },
    "papermill": {
     "duration": 1.818713,
     "end_time": "2021-01-09T13:10:12.921344",
     "exception": false,
     "start_time": "2021-01-09T13:10:11.102631",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_autoencoder(input_dim,output_dim,noise=0.05):\n",
    "    i = Input(input_dim)\n",
    "    encoded = BatchNormalization()(i)\n",
    "    encoded = GaussianNoise(noise)(encoded)\n",
    "    encoded = Dense(640,activation='relu')(encoded)\n",
    "    decoded = Dropout(0.2)(encoded)\n",
    "    decoded = Dense(input_dim,name='decoded')(decoded)\n",
    "    x = Dense(320,activation='relu')(decoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(output_dim,activation='sigmoid',name='label_output')(x)\n",
    "    \n",
    "    encoder = Model(inputs=i,outputs=encoded)\n",
    "    autoencoder = Model(inputs=i,outputs=[decoded,x])\n",
    "    \n",
    "    autoencoder.compile(optimizer=Adam(0.001),loss={'decoded':'mse','label_output':'binary_crossentropy'})\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:10:16.009448Z",
     "iopub.status.busy": "2021-01-09T13:10:16.008407Z",
     "iopub.status.idle": "2021-01-09T13:10:16.011820Z",
     "shell.execute_reply": "2021-01-09T13:10:16.011250Z"
    },
    "papermill": {
     "duration": 1.598049,
     "end_time": "2021-01-09T13:10:16.011941",
     "exception": false,
     "start_time": "2021-01-09T13:10:14.413892",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(hp,input_dim,output_dim,encoder):\n",
    "    inputs = Input(input_dim)\n",
    "    \n",
    "    x = encoder(inputs)\n",
    "    x = Concatenate()([x,inputs]) #use both raw and encoded features\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(hp.Float('init_dropout',0.0,0.5))(x)\n",
    "    \n",
    "    for i in range(hp.Int('num_layers',1,5)):\n",
    "        x = Dense(hp.Int('num_units_{i}',128,256))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Lambda(tf.keras.activations.swish)(x)\n",
    "        x = Dropout(hp.Float(f'dropout_{i}',0.0,0.5))(x)\n",
    "    x = Dense(output_dim,activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs,outputs=x)\n",
    "    model.compile(optimizer=Adam(hp.Float('lr',0.00001,0.1,default=0.001)),loss=BinaryCrossentropy(label_smoothing=hp.Float('label_smoothing',0.0,0.1)),metrics=[tf.keras.metrics.AUC(name = 'auc')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:10:19.812019Z",
     "iopub.status.busy": "2021-01-09T13:10:19.810873Z",
     "iopub.status.idle": "2021-01-09T13:10:21.794435Z",
     "shell.execute_reply": "2021-01-09T13:10:21.793584Z"
    },
    "papermill": {
     "duration": 3.586572,
     "end_time": "2021-01-09T13:10:21.794562",
     "exception": false,
     "start_time": "2021-01-09T13:10:18.207990",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "autoencoder, encoder = create_autoencoder(X.shape[-1],y.shape[-1],noise=0.1)\n",
    "if TRAINING:\n",
    "    autoencoder.fit(X,(X,y),\n",
    "                    epochs=1002,\n",
    "                    batch_size=16384, \n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[EarlyStopping('val_loss',patience=10,restore_best_weights=True)])\n",
    "    encoder.save_weights('./encoder.hdf5')\n",
    "else:\n",
    "    encoder.load_weights('../input/running-00/encoder.hdf5')\n",
    "encoder.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.499367,
     "end_time": "2021-01-09T13:10:24.794715",
     "exception": false,
     "start_time": "2021-01-09T13:10:23.295348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Training the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:10:27.859376Z",
     "iopub.status.busy": "2021-01-09T13:10:27.858212Z",
     "iopub.status.idle": "2021-01-09T13:11:01.067813Z",
     "shell.execute_reply": "2021-01-09T13:11:01.066759Z"
    },
    "papermill": {
     "duration": 34.780645,
     "end_time": "2021-01-09T13:11:01.068036",
     "exception": false,
     "start_time": "2021-01-09T13:10:26.287391",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_fn = lambda hp: create_model(hp,X.shape[-1],y.shape[-1],encoder)\n",
    "\n",
    "tuner = CVTuner(\n",
    "        hypermodel=model_fn,\n",
    "        oracle=kt.oracles.BayesianOptimization(\n",
    "        objective= kt.Objective('val_auc', direction='max'),\n",
    "        num_initial_points=4,\n",
    "        max_trials=60))\n",
    "\n",
    "FOLDS = 5\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "if TRAINING:\n",
    "    gkf = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap=20)\n",
    "    splits = list(gkf.split(y, groups=train['date'].values))\n",
    "    tuner.search((X,),(y,),splits=splits,batch_size=16384,epochs=300,callbacks=[EarlyStopping('val_auc', mode='max',patience=3)])\n",
    "    hp  = tuner.get_best_hyperparameters(1)[0]\n",
    "    pd.to_pickle(hp,f'./best_hp_{SEED}.pkl')\n",
    "    for fold, (train_indices, test_indices) in enumerate(splits):\n",
    "        model = model_fn(hp)\n",
    "        X_train, X_test = X[train_indices], X[test_indices]\n",
    "        y_train, y_test = y[train_indices], y[test_indices]\n",
    "        model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=300,batch_size=16384,callbacks=[EarlyStopping('val_auc',mode='max',patience=10,restore_best_weights=True)])\n",
    "        model.save_weights(f'./model_{SEED}_{fold}.hdf5')\n",
    "        model.compile(Adam(hp.get('lr')/100),loss='binary_crossentropy')\n",
    "        model.fit(X_test,y_test,epochs=6,batch_size=16384)\n",
    "        model.save_weights(f'./model_{SEED}_{fold}_finetune.hdf5')\n",
    "    tuner.results_summary()\n",
    "else:\n",
    "    models = []\n",
    "    hp = pd.read_pickle(f'../input/running-00/best_hp_{SEED}.pkl')\n",
    "    for f in range(FOLDS):\n",
    "        model = model_fn(hp)\n",
    "        if USE_FINETUNE:\n",
    "            model.load_weights(f'../input/running-00/model_{SEED}_{f}_finetune.hdf5')\n",
    "        else:\n",
    "            model.load_weights(f'../input/running-00/model_{SEED}_{f}.hdf5')\n",
    "        model = LiteModel.from_keras_model(model)\n",
    "        models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.50974,
     "end_time": "2021-01-09T13:11:04.350837",
     "exception": false,
     "start_time": "2021-01-09T13:11:02.841097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='Dummy_Env'></a>\n",
    "## Dummy environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:11:07.360635Z",
     "iopub.status.busy": "2021-01-09T13:11:07.358441Z",
     "iopub.status.idle": "2021-01-09T13:11:07.361417Z",
     "shell.execute_reply": "2021-01-09T13:11:07.361939Z"
    },
    "papermill": {
     "duration": 1.508941,
     "end_time": "2021-01-09T13:11:07.362119",
     "exception": false,
     "start_time": "2021-01-09T13:11:05.853178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ENV_REAL = True\n",
    "\n",
    "if (not TRAINING) & (not ENV_REAL):\n",
    "    \n",
    "    test_col = pd.read_pickle(f'../input/dummy-environnement/columns_df_test.pickle')\n",
    "\n",
    "    n_row = 15219\n",
    "\n",
    "    dummy_df = train.iloc[:n_row]\n",
    "\n",
    "    for (index, row) in tqdm(dummy_df.iterrows()):\n",
    "\n",
    "        time.sleep(0.009)\n",
    "        test_df = pd.DataFrame(row).transpose()[test_col]\n",
    "\n",
    "        test_df = pd.DataFrame(row).transpose()\n",
    "        pred_df = pd.DataFrame(columns=['action'], index = [index])\n",
    "\n",
    "        pred_df.action = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.548056,
     "end_time": "2021-01-09T13:11:10.844498",
     "exception": false,
     "start_time": "2021-01-09T13:11:09.296442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='Submission'></a>\n",
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-09T13:11:14.699588Z",
     "iopub.status.busy": "2021-01-09T13:11:14.694506Z",
     "iopub.status.idle": "2021-01-09T13:16:24.464067Z",
     "shell.execute_reply": "2021-01-09T13:16:24.463034Z"
    },
    "papermill": {
     "duration": 312.134712,
     "end_time": "2021-01-09T13:16:24.464207",
     "exception": false,
     "start_time": "2021-01-09T13:11:12.329495",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15219it [05:09, 49.14it/s]\n"
     ]
    }
   ],
   "source": [
    "if (not TRAINING) & (ENV_REAL):\n",
    "    \n",
    "    import janestreet\n",
    "    env = janestreet.make_env()\n",
    "    th = 0.5\n",
    "    \n",
    "    EWM_5000 = RunningEWMean(WIN_SIZE = 5000,n_size = 2)\n",
    "    EWM_10000 = RunningEWMean(WIN_SIZE = 10000,n_size = 2)\n",
    "    EWM_20000 = RunningEWMean(WIN_SIZE = 20000,n_size = 2)\n",
    "\n",
    "    train_FE = []\n",
    "    \n",
    "    \n",
    "    for (test_df, pred_df) in tqdm(env.iter_test()):\n",
    "        \n",
    "        EWM_5000.push(np.float64(np.array(test_df[['feature_0','feature_1']])))\n",
    "        EWM_10000.push(np.float64(np.array(test_df[['feature_0','feature_1']])))\n",
    "        EWM_20000.push(np.float64(np.array(test_df[['feature_0','feature_1']])))\n",
    "\n",
    "        FE = []\n",
    "\n",
    "        FE = {\n",
    "            'feature_0_EWM_5000' : EWM_5000.get_mean()[0][0],\n",
    "            'feature_1_EWM_5000' : EWM_5000.get_mean()[0][1],\n",
    "            'feature_0_EWM_10000' : EWM_10000.get_mean()[0][0],\n",
    "            'feature_1_EWM_10000' : EWM_10000.get_mean()[0][1],\n",
    "            'feature_0_EWM_20000' : EWM_20000.get_mean()[0][0],\n",
    "            'feature_1_EWM_20000' : EWM_20000.get_mean()[0][1],\n",
    "        }\n",
    "\n",
    "        test_df_FE = pd.concat([test_df,pd.DataFrame(FE, index=[test_df.index[0]])],axis=1)\n",
    "\n",
    "        if test_df_FE['weight'].item() > 0:\n",
    "            x_tt = test_df_FE.loc[:, features].values\n",
    "            if np.isnan(x_tt[:, 1:].sum()):\n",
    "                x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n",
    "            pred = np.mean([model.predict(x_tt) for model in models],axis=0)\n",
    "            pred = np.mean(pred)\n",
    "            pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n",
    "        else:\n",
    "            pred_df.action = 0\n",
    "            \n",
    "        env.predict(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.606655,
     "end_time": "2021-01-09T13:16:29.626833",
     "exception": false,
     "start_time": "2021-01-09T13:16:27.020178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 1011.244579,
   "end_time": "2021-01-09T13:16:32.949416",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-09T12:59:41.704837",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
